# -*- coding: utf-8 -*-
"""Fraud_Detection_Complete.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sfWwA7h1_Hye0FdxkPeDQU1vTjYnJWHK
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import matplotlib.gridspec as gridspec
import seaborn as sns

from sklearn.base import BaseEstimator,TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split,cross_val_predict,cross_val_score, GridSearchCV,RandomizedSearchCV
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.metrics import confusion_matrix,classification_report,f1_score,recall_score,precision_score,accuracy_score,precision_recall_curve,roc_curve,roc_auc_score

from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier

from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTEENN
from collections import Counter

from scipy.stats import norm, multivariate_normal

plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12

import warnings
warnings.filterwarnings('ignore')

import random
random.seed(0)

def data_preparation(data):
    features = data.iloc[:,0:-1]
    label = data.iloc[:,-1]
    x_train,x_test,y_train,y_test = train_test_split(features,label,test_size=0.2,random_state=0)

    #Standarad scaler is not applied since all the features are outcomes of PCA and are already normalized.
    #sc = StandardScaler()
    #x_train = sc.fit_transform(x_train)
    #x_test = sc.transform(x_test)

    print("Length of training data",len(x_train))
    print("Length of test data",len(x_test))
    return x_train,x_test,y_train,y_test

def build_model_train_test(model,x_train,x_test,y_train,y_test):
    model.fit(x_train,y_train)

    y_pred = model.predict(x_train)

    print("\n----------Accuracy Scores on Train data------------------------------------")
    #print("Clasification Report")
    #print(classification_report(y_train,y_pred))

    print("F1 Score: ", f1_score(y_train,y_pred))
    print("Precision Score: ", precision_score(y_train,y_pred))
    print("Recall Score: ", recall_score(y_train,y_pred))

    print("\n----------Accuracy Scores on Cross validation data------------------------------------")
    y_pred_cv = cross_val_predict(model,x_train,y_train,cv=5)
    print("F1 Score: ", f1_score(y_train,y_pred_cv))
    print("Precision Score: ", precision_score(y_train,y_pred_cv))
    print("Recall Score: ", recall_score(y_train,y_pred_cv))


    print("\n----------Accuracy Scores on Test data------------------------------------")
    y_pred_test = model.predict(x_test)

    #print("Clasification Report")
    #print(classification_report(y_test,y_pred_test))

    print("F1 Score: ", f1_score(y_test,y_pred_test))
    print("Precision Score: ", precision_score(y_test,y_pred_test))
    print("Recall Score: ", recall_score(y_test,y_pred_test))

    #Confusion Matrix
    plt.figure(figsize=(18,6))
    gs = gridspec.GridSpec(1,2)

    ax1 = plt.subplot(gs[0])
    cnf_matrix = confusion_matrix(y_train,y_pred)
    row_sum = cnf_matrix.sum(axis=1,keepdims=True)
    cnf_matrix_norm =cnf_matrix / row_sum
    sns.heatmap(cnf_matrix_norm,cmap='YlGnBu',annot=True)
    plt.title("Normalized Confusion Matrix - Train Data")

#     ax2 = plt.subplot(gs[1])
#     cnf_matrix = confusion_matrix(y_train,y_pred_cv)
#     sns.heatmap(cnf_matrix,cmap='YlGnBu',annot=True,fmt='d')
#     plt.title("Confusion Matrix - CV Data")

    ax3 = plt.subplot(gs[1])
    cnf_matrix = confusion_matrix(y_test,y_pred_test)
    row_sum = cnf_matrix.sum(axis=1,keepdims=True)
    cnf_matrix_norm =cnf_matrix / row_sum
    sns.heatmap(cnf_matrix_norm,cmap='YlGnBu',annot=True)
    plt.title("Normalized Confusion Matrix - Test Data")

def build_model_train(model,x_train,y_train):
    model.fit(x_train,y_train)
    y_pred = model.predict(x_train)

    #print("Clasification Report")
    #print(classification_report(y_train,y_pred))

    print("\n----------Accuracy Scores on Train data------------------------------------")
    print("F1 Score: ", f1_score(y_train,y_pred))
    print("Precision Score: ", precision_score(y_train,y_pred))
    print("Recall Score: ", recall_score(y_train,y_pred))

    print("\n----------Accuracy Scores on Cross validation data------------------------------------")
    y_pred_cv = cross_val_predict(model,x_train,y_train,cv=5)
    print("F1 Score: ", f1_score(y_train,y_pred_cv))
    print("Precision Score: ", precision_score(y_train,y_pred_cv))
    print("Recall Score: ", recall_score(y_train,y_pred_cv))

    cnf_matrix = confusion_matrix(y_train,y_pred)
    row_sum = cnf_matrix.sum(axis=1,keepdims=True)
    cnf_matrix_norm =cnf_matrix / row_sum
    sns.heatmap(cnf_matrix_norm,cmap='YlGnBu',annot=True)
    plt.title("Normalized Confusion Matrix - Train data")

def build_model_test(model,x_test,y_test):

    y_pred_test = model.predict(x_test)
    #print("Clasification Report - Test Data")
    #print(classification_report(y_test,y_pred_test))

    print("F1 Score: ", f1_score(y_test,y_pred_test))
    print("Precision Score: ", precision_score(y_test,y_pred_test))
    print("Recall Score: ", recall_score(y_test,y_pred_test))

    cnf_matrix = confusion_matrix(y_test,y_pred_test)
    row_sum = cnf_matrix.sum(axis=1,keepdims=True)
    cnf_matrix_norm =cnf_matrix / row_sum
    sns.heatmap(cnf_matrix_norm,cmap='YlGnBu',annot=True)
    plt.title("Normalized Confusion Matrix - Test data")

def SelectThresholdByCV(probs,y):
    best_threshold = 0
    best_f1 = 0
    f = 0
    precision =0
    recall=0
    best_recall = 0
    best_precision = 0
    precisions=[]
    recalls=[]

    thresholds = np.arange(0.0,1.0,0.001)
    for threshold in thresholds:
        predictions = (probs > threshold)
        f = f1_score(y, predictions)
        precision = precision_score(y, predictions)
        recall = recall_score(y, predictions)
        #print("Threshold {0},Precision {1},Recall {2}".format(threshold,precision,recall))

        if f > best_f1:
            best_f1 = f
            best_precision = precision
            best_recall = recall
            best_threshold = threshold

        precisions.append(precision)
        recalls.append(recall)

    #Precision-Recall Trade-off
    plt.plot(thresholds,precisions,label='Precision')
    plt.plot(thresholds,recalls,label='Recall')
    plt.xlabel("Threshold")
    plt.title('Precision Recall Trade Off')
    plt.legend()
    plt.show()

    print ('Best F1 Score %f' %best_f1)
    print ('Best Precision Score %f' %best_precision)
    print ('Best Recall Score %f' %best_recall)
    print ('Best Epsilon Score', best_threshold)

def SelectThresholdByCV_Anomaly(probs,y):
    best_epsilon = 0
    best_f1 = 0
    f = 0
    precision =0
    recall=0
    best_recall = 0
    best_precision = 0

    epsilons = sorted(np.unique(probs))
    #print(epsilons)

    precisions=[]
    recalls=[]
    for epsilon in epsilons:
        predictions = (probs < epsilon)
        f = f1_score(y, predictions)
        precision = precision_score(y, predictions)
        recall = recall_score(y, predictions)
        #print("Theshold {0},Precision {1},Recall {2}".format(epsilon,precision,recall))

        if f > best_f1:
            best_f1 = f
            best_precision = precision
            best_recall = recall
            best_epsilon = epsilon

        precisions.append(precision)
        recalls.append(recall)

    #Precision-Recall Trade-off
    plt.plot(epsilons,precisions,label='Precision')
    plt.plot(epsilons,recalls,label='Recall')
    plt.xlabel("Epsilon")
    plt.title('Precision Recall Trade Off')
    plt.legend()
    plt.show()

    print ('Best F1 Score %f' %best_f1)
    print ('Best Precision Score %f' %best_precision)
    print ('Best Recall Score %f' %best_recall)
    print ('Best Epsilon Score', best_epsilon)

def Print_Accuracy_Scores(y,y_pred):
    print("F1 Score: ", f1_score(y,y_pred))
    print("Precision Score: ", precision_score(y,y_pred))
    print("Recall Score: ", recall_score(y,y_pred))

#Loading Dataset
cc_dataset = pd.read_csv("creditcard.csv")

cc_dataset.shape

cc_dataset.head()

cc_dataset.describe()

#Code for checking if any feature has null values
cc_dataset.isnull().any()

#Counts for each class in the dataset. As you can see, we have only 492 (0.17%) fraud cases out of 284807 records. Remaining 284315 (99.8%) of the records belong to genuine cases.
cc_dataset['Class'].value_counts()

#Data Visualization for checking the distribution for Genuine cases & Fraud cases for each feature
v_features = cc_dataset.columns
plt.figure(figsize=(12,31*4))
gs = gridspec.GridSpec(31,1)

for i, col in enumerate(v_features):
    ax = plt.subplot(gs[i])
    sns.distplot(cc_dataset[col][cc_dataset['Class']==0],color='g',label='Genuine Class')
    sns.distplot(cc_dataset[col][cc_dataset['Class']==1],color='r',label='Fraud Class')
    ax.legend()
plt.show()

"""Feature selection:
    1) We can see Normal Distribution of anomalous transactions (class = 1) is matching with Normal Distribution of genuine transactions (class = 0) for V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8' features. It is better to delete these features as they may not be useful in finding anomalous records.
    2) Time is also not useful variable since it contains the seconds elapsed between the transaction for that record and the first transaction in the dataset. So the data is in increasing order always.

However we'll delete these features only after building a dumb model.
"""

#Splitting the input features and target label into different arrays
X = cc_dataset.iloc[:,0:-1]
Y = cc_dataset.iloc[:,-1]
X.columns

#Train Test split - By default train_test_split does STRATIFIED split based on label (y-value).
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=0)

#Feature Scaling - Standardizing the scales for all x variables
#PN: We should apply fit_transform() method on train set & only transform() method on test set
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)



"""# Approach 1 - Adhoc Approach

In this approach, we applied mutiple models on the data, did error analysis & tried to improve model performance.

Step1 - SGDClassifier model

Step2 - Ensemble Models

# Step1 - SGDClassifier
"""

#Build a basic SGDClassifier model
sgd_cls = SGDClassifier(class_weight='balanced', random_state=0)
sgd_cls.fit(x_train,y_train)

#Predict target variable (Y-label)
y_pred = sgd_cls.predict(x_train)

"""Since the data set is imbalanced, we should go with f1-score for comparing models & shouldn’t use accuracy_score() & roc_auc_score() for the below reasons.

    1) accuracy_score looks into how many records are predicted correctly. So in the imbalance data set, even if we predict all records as 0 (genuine class) the accuracy score will still be more than 99%.
    2) roc_auc_score is the plot between TRUE POSITIVE RATE (recall/sensitivity) and FALSE POSITIVE RATE (1 - true negative rate). Since it focuses on both positives and negatives equally, it works well for the balanced datasets and doesn’t work well for imbalanced datasets.
    3) F1-Score is a harmonic mean of precision and recall (harmonic mean gives more weight to low value). As a result, the classifier will give high F1-Score only if both Precision and Recall are high. F1-Score focuses more on true positive cases than true negative cases. So we should use F1-Score to compare multiple models as the positive case is rare.

For Fraud detection data, recall score is more important than precison score, since the classifier should classify all the fraud cases correctly (High Recall). However it is OK to incorrectly predict some genuine classes as fraud classes.

<img height="600" width="750" src="precision_recall.png">
"""

#Classification_report provides main classification metrics(precision, recall & f1-score) for each class
print(classification_report(y_train,y_pred))

Print_Accuracy_Scores(y_train,y_pred)

"""From the basic SGDClassifier model we got a very good score for Recall but Precision is very low. So we need to approach GridSearchCV to tune and find best hyper-parameter values.

Note: We have commented GridSearchCV code as it is taking long time to execute. So we took only couple of hyper-parameters & build the best model
"""

#Build the best model out of GridSearchCV
best_sgd_clf = SGDClassifier(alpha=5, class_weight='balanced',random_state=0)
build_model_train_test(best_sgd_clf,x_train,x_test,y_train,y_test)

"""By looking at the performance measures on Test data, So we can conclude that SGDClassifier has provided good results on both train and test data sets with F1, Precision and Recall scores around 80%

Note: For fraud detection, recall score is more important than precision score.

I've used class_weight='balanced' parameter for all the models since the data is imbalanced. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as "n_samples / (n_classes * np.bincount(y))". So the model applies more weights to rare classes.

Confusion Matrix - Normalization can be interesting in case of class imbalance to have a more visual interpretation of which class is being misclassified. You can see from the above normalized confusion matrix that 20% of the Fraud records are misclassified in this model. So there is still lot of scope for improvement.
"""

featimp = pd.Series(best_sgd_clf.coef_.ravel(),index=X.columns).sort_values(ascending=False)
print(featimp)

"""We can see that coefficients for SGDClassifier model are almost close to zero for the features: V27,Amount,V28,V19,V20,V25,V26,V15,V22,Time,V5,V24,V13,V8"""

#Decision scores or Confidence scores - Is a measure of distance of that sample to decision boundary hyperplane
y_pred_decision = best_sgd_clf.decision_function(x_train)

#Precision recall curve - Computes precision-recall pairs for different probability thresholds
precisions, recalls, thresholds = precision_recall_curve(y_train,y_pred_decision)

plt.plot(recalls,precisions)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision Recall Curve')

#Precision-Recall Trade-off
plt.plot(thresholds,precisions[:-1],label='Precision')
plt.plot(thresholds,recalls[:-1],label='Recall')

plt.xlabel("Decision Score")
plt.title('Precision Recall Trade Off')
plt.legend()
plt.xlim([-10,20])
plt.show()

"""Precision Recall Trade Off also suggests that decision score of 0 provides better accuracy scores for Precision and Recall. So we can assume this is the best model from SGDClassifier.

For Demonstration purpose, I created ROC curve as well below. However ROC AUC score is not a good measure for imbalance data sets as discussed above.
You can notice that the above model has given us the AUC score of 0.98, which is very high.
"""

fpr, tpr, thesholds = roc_curve(y_train,y_pred_decision)

plt.plot(fpr,tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.show()

roc_auc_score(y_train,y_pred_decision)



"""# Step2 - Ensemble Models

Ensemble models may handle imbalanced datasets better due to their nature of building multiple models randomly and aggregate their predictions.
"""

# #Use GridSearchCV to find the best parameters for RandomForest algorithm
# param_grid = {
#     'n_estimators':[50,100,200],
#     'max_depth':[4,6,8,10],
#     'min_samples_split':[5,10,20,50]
# }

# rnd_clf = RandomForestClassifier(criterion='gini',class_weight='balanced', n_jobs=-1, random_state=0)
# grid_search = GridSearchCV(rnd_clf,param_grid,cv=5,scoring='f1_weighted')
# grid_search.fit(x_train,y_train)

# grid_search.best_estimator_

#Best estimator of random forest
rnd_clf = RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=10, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=5, min_weight_fraction_leaf=0.0,
            n_estimators=50, n_jobs=-1, oob_score=False, random_state=0,
            verbose=0, warm_start=False)
build_model_train_test(rnd_clf,x_train,x_test,y_train,y_test)

"""By looking at the performance measures, we can say that Random Forest is definitely giving us better accuracy than SGDClassifier. So I'm going to explore more on Random Forest model further."""

#Check the Feature importance scores from Random Forest model
featimp = pd.Series(rnd_clf.feature_importances_,index=X.columns).sort_values(ascending=False)
print(featimp)

"""We can see that most of the features we have decided to remove after looking at the distribution earlier are having low importance scores in random forest model & also coefficients close to zero at SGCClassifier. Let us remove these features for further analysis."""

cc_dataset.drop(labels = ['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8','Time'], axis = 1, inplace=True)
cc_dataset.columns

cc_dataset.describe()

sns.distplot(cc_dataset['Amount'],bins=10)

"""The feature 'Amount' has higher standard deviation of 250, which indicate the spread is very high & also we might have outliers in the data. So let us go for feature scaling for Amount variable using StandardScaler()."""

cc_dataset['Amount'] = StandardScaler().fit_transform(cc_dataset['Amount'].reshape(-1,1))
cc_dataset.describe()

sns.distplot(cc_dataset['Amount'][cc_dataset['Class']==0],color='g',label='Genuine Class')
sns.distplot(cc_dataset['Amount'][cc_dataset['Class']==1],color='r',label='Fraud Class')

#Data Preparation
x_train,x_test,y_train,y_test = data_preparation(cc_dataset)

#Let us check the performance of RandomForest after removing few features
build_model_train_test(rnd_clf,x_train,x_test,y_train,y_test)

"""You can notice that RandomForest has given pretty decent performance on this dataset though it is overfitting little bit. Also notice that same accuracy is maintained even after removing the features ('V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8','Time').

I developed KNN algorithm here, mainly to use it in Voting Classifier. But KNN is giving us pretty good performance as well.
"""

knn_clf = KNeighborsClassifier(n_neighbors=3)
build_model_train_test(knn_clf,x_train,x_test,y_train,y_test)

"""Let us also try AdaBoostClassifier before using voting classifier.
Looking at below scores, we can say that AdaBoost is overfitting on the training data. Overall a good performance on the cross validation and test dataset nevertheless.
"""

ada_clf = AdaBoostClassifier(
        DecisionTreeClassifier(max_depth=3,class_weight='balanced'), n_estimators=100,
        algorithm="SAMME.R", learning_rate=0.5, random_state=0
    )
build_model_train_test(ada_clf,x_train,x_test,y_train,y_test)

"""RandomForest, AdaBoost & KNN are all providing good accuracy measures. So let us include all of them in a soft voting classifier and see if we can improve the performance even further."""

soft_voting_clf = VotingClassifier(
    estimators=[('rf', rnd_clf), ('ada', ada_clf), ('knn',knn_clf)],
    voting='soft')

build_model_train_test(soft_voting_clf,x_train,x_test,y_train,y_test)

"""I'm pretty happy with the outcome of Voting Classifier here though it is over fitting on the train data set. But a deviation of 5% is acceptable.
I'd like to improve the recall score further. So let us use predict_proba to get the probabilities and see if we can improve the recall by using a different threshold.
"""

#Probability scores for each record for prediction
probs_sv_test = soft_voting_clf.predict_proba(x_test)

#Choose the best probability threshold where F1-score is highest
SelectThresholdByCV(probs_sv_test[:,1],y_test)

y_pred_test = (probs_sv_test[:,1] > 0.571)
Print_Accuracy_Scores(y_test,y_pred_test)

"""However we couldn't improve the recall score much on the test data. Precision is going down drastically if we try to improve recall little bit. So I'd imagine that the model has given us best results with 83 recall score and 95 precision score at 0.57 threshold.

Conclusion: From adhoc approach, we have seen slightly better results for Voting Classifier than RandomForest. So we could go with Voting Classifier with F1-score of 89 and Recall score of 83.
However we are going to check the performance of both Voting Classifier and RandomForest in our next approach using resampling technique.
"""



"""# Approach 2 - Resampling Techniques

In this approach we are going to balance the data using resampling techniques. The main objective of resampling is to balance classes by either increasing the frequency of minority class or decreasing the frequency of majority class.

We can use imbalanced-learn library of python to resample the data. But we are going to use manual approach first for the demonstration purpose and see the synthetic approach later on using oversampling.

# Step1 - Under Sampling

Here we randomly choose only a sample of genuine records, which will make sure to balance the instances of majority and minority classes.
"""

genuine_data = cc_dataset[cc_dataset['Class']==0]
fraud_data = cc_dataset[cc_dataset['Class']==1]

genuine_indexes = genuine_data.index
fraud_indexes = fraud_data.index
count_genuine = len(genuine_data)
count_fraud = len(fraud_data)

#Choose a smaple of genuine records which is equal to number of fraud records
genuine_indexes_undersample = np.random.choice(genuine_indexes,count_fraud,replace=False)
print(len(genuine_indexes_undersample))

#Merge under-sampled genuine records with fraud records to get the final under-sampled dataset
undersample_indexes = np.concatenate((genuine_indexes_undersample,fraud_indexes),axis=0)
print(len(undersample_indexes))

undersample_data = cc_dataset.iloc[undersample_indexes,:].values

#Split X & Y
us_data_x = undersample_data[:,0:-1]
us_data_y = undersample_data[:,-1]

#Model building on under-sampled train data and then check the accuracy scores on train, test data.
#Note: Class_weight='balanced' parameter is removed for the model on under-sample since the training data is balanced after using under-sampling technique.
rnd_clf = RandomForestClassifier(n_estimators=100,criterion='gini', n_jobs=-1, random_state=0)
#Train the model on under-sampled data and check the performance on actual test data
build_model_train_test(rnd_clf,us_data_x,x_test,us_data_y,y_test)

"""You can notice that precision has gone down miserably on the original imbalanced test data when we trained the model on under-sampled data. This is due to potential loss of information/examples during the training. So the under-sampled data will not be an accurate representation of the population. Thereby, resulting in inaccurate results with the actual test data set."""



"""# Step2 - Over Sampling

Over-Sampling increases the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample.
"""

y_train = y_train.reshape(-1,1) #Reshaping Y to 2 dimensional array

data_train = np.hstack((x_train,y_train)) #Stack Y label with input X array
print("Length of Training Data",len(data_train))
genuine_data = data_train[data_train[:,-1]==0]
print("Length of Genuine Data",len(genuine_data))
fraud_data = data_train[data_train[:,-1]==1]
print("Length of Fraud Data",len(fraud_data))

#Code for adding the fraud records 200 times to the training set. So overall we are able to get 75:25 ratio of genuine to fraud class in the over-sampled data.
os_data = genuine_data.copy()
for i in range(200):
    os_data = np.vstack((os_data,fraud_data))

print("Length of Training Data",len(os_data))
print("Length of Genuine Data",len(os_data[os_data[:,-1]==0]))
print("Length of Fraud Data",len(os_data[os_data[:,-1]==1]))

#Split X & Y
os_data_x = os_data[:,0:-1]
os_data_y = os_data[:,-1]

#Model building on over-sampled train data and then check the accuracy scores on train and test data.
rnd_clf = RandomForestClassifier(n_estimators=100,criterion='gini',class_weight='balanced',n_jobs=-1, random_state=0)
#Train the model on over-sampled data and check the performance on actual test data
build_model_train_test(rnd_clf,os_data_x,x_test,os_data_y,y_test)

#Probability score for each record for prediction
probs_rf_test = rnd_clf.predict_proba(x_test)

#Choose the best probability threshold where F1-score is highest
SelectThresholdByCV(probs_rf_test[:,1],y_test)

y_pred_test = (probs_rf_test[:,1] > 0.32)
Print_Accuracy_Scores(y_test,y_pred_test)

"""Using RandomForest Classifier, we could improve recall from 81 to 85 when we reduce the threshold to 0.349 with slightly lower precision score. However if we try to increase recall further above 85, precision score is going down pretty quickly. So I choose the threshold of 0.32, which is giving us reasonable recall and precision scores.

Note: Over-sampled data has given slightly better results than the original training data (F1 score of 88 vs 84) with slight increase in both recall and precision scores.

Conclusion: Over-sampling fraud records to 25% of overall training samples is providing best results when combined with class_weight='balanced' parameter for RandomForest.
"""

#Let us check the cross validation scores on the original train data set
cv_score = cross_val_score(rnd_clf,x_train,y_train,cv=5,scoring='f1')
print("Average F1 score CV",cv_score.mean())

cv_score = cross_val_score(rnd_clf,x_train,y_train,cv=5,scoring='recall')
print("Average Recall score CV",cv_score.mean())

"""From the above cross validation scores, it looks like the RandomForest model is overfitting little bit. We will use the scores on cross validation for choosing the best model.

Let us check the performance of Soft Voting Classifier as well on this over-sampled data.
"""

rnd_clf = RandomForestClassifier(n_estimators=100,criterion='gini',class_weight='balanced', n_jobs=-1, random_state=0)

ada_clf = AdaBoostClassifier(
        DecisionTreeClassifier(max_depth=3,class_weight='balanced'), n_estimators=100,
        algorithm="SAMME.R", learning_rate=0.5, random_state=0
    )
knn_clf = KNeighborsClassifier(n_neighbors=3)

soft_voting_clf = VotingClassifier(
    estimators=[('rf', rnd_clf), ('ada', ada_clf), ('knn',knn_clf)],
    voting='soft')

#Train the model on oversampled data and check the performance on actual test data
build_model_train_test(soft_voting_clf,os_data_x,x_test,os_data_y,y_test)

#Probability scores for each record for prediction
probs_sv_test = soft_voting_clf.predict_proba(x_test)

#Choose the best probability threshold where F1-score is highest
SelectThresholdByCV(probs_sv_test[:,1],y_test)

y_pred_test = (probs_sv_test[:,1] > 0.546)
Print_Accuracy_Scores(y_test,y_pred_test)

"""Using Soft Voting Classifier, recall is improving little bit when we reduce the threshold. However precision is going down pretty quickly to improve little bit of recall. So I choose the threshold of 0.546, which is giving us reasonable recall (in line with the original model) and the best precision score.

Note: Oversampled data has given slightly better results than the original data (F1 score of 89 vs 88) with slight increase in recall and precision.

So we can conclude that over-sampling fraud records to 25% of overall training samples is providing best results when combined with class_weight='balanced' parameter using soft voting classifier.
"""

#Let us check the cross validation scores on the original train data set
cv_score = cross_val_score(soft_voting_clf,x_train,y_train,cv=5,scoring='f1')
print("Average F1 score CV",cv_score.mean())

cv_score = cross_val_score(soft_voting_clf,x_train,y_train,cv=5,scoring='recall')
print("Average Recall score CV",cv_score.mean())

"""Conclusion: Both RandomForest and Voting Classifier have given us similar results on the over-sampled data using manual process(projecting fraud cases to 25% of the total  training examples).  

However Voting Classifier has given better results on K-fold cross validation on the original train dataset. So I'd go with Voting Classifier as the final model selection using manual process of over-sampling.
"""



"""# Step3 - Over Sampling using SMOTE

imbalanced-learn is a python package offering a number of re-sampling techniques commonly used in datasets showing strong between-class imbalance. It is compatible with scikit-learn and is part of scikit-learn-contrib projects.

imbalanced-learn is currently available on the PyPi's repository and you can install it via pip: pip install -U imbalanced-learn

Here we are going to explore Synthetic Minority Oversampling Technique (SMOTE).
"""

os = SMOTE(random_state=0)

#Generate the oversample data
os_res_x,os_res_y=os.fit_sample(x_train,y_train)
#Counts of each class in oversampled data
print(sorted(Counter(os_res_y).items()))

"""We can see that fraud records are imputed and brought close to genuine records in this oversampled data using SMOTE. Hence both classes are equally distributed now."""

#RandomForest for training over-sampled data set.
#Note: we have removed class_weight='balanced' since the data is balanced using SMOTE.
rnd_clf = RandomForestClassifier(n_estimators=100,criterion='gini',n_jobs=-1, random_state=0)
#Train the model on oversampled data and check the performance on actual test data
build_model_train_test(rnd_clf,os_res_x,x_test,os_res_y,y_test)

#Probability scores for each record for prediction
probs_rf_test = rnd_clf.predict_proba(x_test)

#Choose the best probability threshold where F1-score is highest
SelectThresholdByCV(probs_rf_test[:,1],y_test)

y_pred_test = (probs_rf_test[:,1] > 0.63)
Print_Accuracy_Scores(y_test,y_pred_test)

"""We could improve the precision score by changing the threshold to 0.63"""

#Let us check cross validation scores on the orginal train data
cv_score = cross_val_score(rnd_clf,x_train,y_train,cv=5,scoring='f1')
print("Average F1 score CV",cv_score.mean())

cv_score = cross_val_score(rnd_clf,x_train,y_train,cv=5,scoring='recall')
print("Average Recall score CV",cv_score.mean())

"""RandomForest has almost the same results on test data for both manual and synthetic over-sampling approaches: F1 score of 88 using manual approach vs 86 using synthetic approach and both the approaches providing 85 recall score.

However on the cross validation, Synthetic approach (F1-score of 85 and Recall score of 78) has better scores compared to manual approach (F1-score of 82 and Recall score of 73).

So SMOTE has an edge over manual over-sampling approach for RandomForest.

Let us check the performance of Soft Voting Classifier on this over-sampled data.
"""

rnd_clf = RandomForestClassifier(n_estimators=100,criterion='gini', n_jobs=-1, random_state=0)

ada_clf = AdaBoostClassifier(
        DecisionTreeClassifier(max_depth=3), n_estimators=100,
        algorithm="SAMME.R", learning_rate=0.5, random_state=0
    )
knn_clf = KNeighborsClassifier(n_neighbors=3)

soft_voting_clf = VotingClassifier(
    estimators=[('rf', rnd_clf), ('ada', ada_clf), ('knn',knn_clf)],
    voting='soft')

#Train the model on oversampled data and check the performance on actual test data
build_model_train_test(soft_voting_clf,os_res_x,x_test,os_res_y,y_test)

"""Precision score has gone down on test data using Synthetic over-sampled data approach. This could be because of the model parameters. Let us try to change the threshold and see if we can improve precision while still maintaining the recall score around 85."""

#Probability scores for each record for prediction
probs_sv_test = soft_voting_clf.predict_proba(x_test)

#Choose the best probability threshold where F1-score is highest
SelectThresholdByCV(probs_sv_test[:,1],y_test)

y_pred_test = (probs_sv_test[:,1] > 0.718)
Print_Accuracy_Scores(y_test,y_pred_test)

#Let us check cross validation scores on the original train data
cv_score = cross_val_score(soft_voting_clf,x_train,y_train,cv=5,scoring='f1')
print("Average F1 score CV",cv_score.mean())

cv_score = cross_val_score(soft_voting_clf,x_train,y_train,cv=5,scoring='recall')
print("Average Recall score CV",cv_score.mean())

"""Voting Classifier has given almost the same results on test data and cross validation for both manual over-sampling and synthetic over-sampling. PN: Accuracy is almost same using adhoc approach as well for Voting classifier. (F1 score of 89 and recall score of 83). So Voting classifier has performed consistently in all 3 trials we did so far.

Whereas RandomForest is not far behind too and given the same accuracy scores as Voting Classifier using SMOTE.

So I'd recommend using SMOTE approach and choose either RandomForest or Soft Voting Classifier as the final model.
"""



"""# Approach 3 - Anomaly Detection using Gaussian (Normal) Distribution

For training and evaluating Gaussian distribution algorithms, we are going to split the train, cross validation and test data sets using blow ratios.
    1) Train:  60% of the Genuine records (y=0), no Fraud records(y=1). So the training set will not have a label as well.
    2) CV:  20% of the Genuine records (y=0), 50% of the Fraud records(y=1)
    3) Test: Remaining 20% of the Genuine records(y=0), Remaining 50% of the Fraud records(y=1)

Procedure for anomaly detection:
    1) Fit the model p(x) on training set.
    2) On cross validation/test data, predict
        y = 1 if p(x) < epsilon (anomaly)
        y = 0 if p(x) >= epsilon (normal)
    3) We use cross validation to choose parameter epsilon using the evaluation metrics Preceion/Recall, F1-score.

We could use couple of Gaussian distribution models for training anomaly detection.
    1) Gaussian (Normal) Distribution - the normal distribution is parametrized in terms of the mean and the variance.
    2) Multivariate Normal Distribution - The probability density function for multivariate_normal is parametrized in terms of the mean and the covariance.

For this dataset, we are going to use multivariate normal probability density function, since it automatically generates the relationships (correlation) between variables to calculate the probabilities. So we don't need to derive new features.
As the features are outcome of PCA, it is difficult for us to understand the relationship between these features.

However multivariate normal probability density function is computationally expensive compared to normal Gaussian probability density function. On very large datasets, we might have to prefer Gaussian probability density function instead of multivariate normal probability density function to speed up the process and do feature engineering based on the subject matter expertise.

Features that we choose for these algorithms have to be normally distributed. Otherwise we need to transform the features to normal distribution using log, sqrt etc.

Choose features that might take on unusually large or small values in the event of an anomaly. We looked at the distribution in the beginning using distplot. So it is wise to choose features which have completely different distribution for fraud records compared to genuine records.
"""

#Loading Dataset
cc_dataset = pd.read_csv("D:/HRao/TechDocs/DataScience/MLPython_DataJango/Notes/data/creditcard.csv")

"""Below features has almost same distribution for both genuine & fraud records. So they wouldn't be of much help in anomaly detection algorithm."""

cc_dataset.drop(labels = ['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8','Time'], axis = 1, inplace=True)

"""Below features doesn't have the same distribution for both genuine & fraud records. However distribution for fraud records is not unusual as well.
So I'll delete these features as well, since the features with unusual behavior for Fraud records will be most useful in anomaly detection algorithm.
"""

cc_dataset.drop(labels = ['V1','V2','V5','V6','V7','V21','Amount'], axis = 1, inplace=True)
cc_dataset.columns

#Visualization to understand the relationship between features and also data pattern using pair plot from seaborn
g = sns.pairplot(cc_dataset,hue="Class",diag_kind='kde')

"""There is not much insight form the pairplot except that most of features have clear separation for fraud records versus genuine records. We can notice that distribution of fraud records is quite different compared to genuine records in the diagonal kde plots."""

#Data Visualization for checking the normal distribution
v_features = cc_dataset.columns
plt.figure(figsize=(12,12*4))
gs = gridspec.GridSpec(12,1)

for i, col in enumerate(v_features):
    ax = plt.subplot(gs[i])
    sns.kdeplot(cc_dataset[col])
plt.show()

"""All the features looks to be normally distributed. So we can train the Multivariate Guassian Distribution algorthm using the original features."""

#Example for applying log tranformation
#cc_dataset['V4'] = cc_dataset['V4'].apply(lambda x: np.log((x**2)+10))
#sns.kdeplot(cc_dataset['V4'])

#Example for applying log tranformation
#cc_dataset['V11'] = cc_dataset['V11'].apply(lambda x: np.log((x**2)+50))
#sns.kdeplot(cc_dataset['V11'])

def estimateGaussian(data):
    mu = np.mean(data,axis=0)
    sigma = np.cov(data.T)
    return mu,sigma

def MultivariateGaussianDistribution(data,mu,sigma):
    p = multivariate_normal.pdf(data, mean=mu, cov=sigma)
    p_transformed = np.power(p,1/100) #transformed the probability scores by p^1/100 since the values are very low (up to e-150)
    return p_transformed

genuine_data = cc_dataset[cc_dataset['Class']==0]
fraud_data = cc_dataset[cc_dataset['Class']==1]

#Split Genuine records into train & test - 60:40 ratio
genuine_train,genuine_test = train_test_split(genuine_data,test_size=0.4,random_state=0)
print(genuine_train.shape)
print(genuine_test.shape)

#Split 40% of Genuine Test records into Cross Validation & Test again (50:50 ratio)
genuine_cv,genuine_test = train_test_split(genuine_test,test_size=0.5,random_state=0)
print(genuine_cv.shape)
print(genuine_test.shape)

#Split Fraud records into Cross Validation & Test (50:50 ratio)
fraud_cv,fraud_test = train_test_split(fraud_data,test_size=0.5,random_state=0)

#Drop Y-label from Train data
train_data = genuine_train.drop(labels='Class',axis=1)
print(train_data.shape)

#Cross validation data
cv_data = pd.concat([genuine_cv,fraud_cv])
cv_data_y = cv_data['Class']
cv_data.drop(labels='Class',axis=1,inplace=True)
print(cv_data.shape)

#Test data
test_data = pd.concat([genuine_test,fraud_test])
test_data_y = test_data['Class']
test_data.drop(labels='Class',axis=1,inplace=True)
print(test_data.shape)

#StandardScaler – Feature scaling is not required since all the features are already standardized via PCA
#sc = StandardScaler()
#train_data = sc.fit_transform(train_data)
#cv_data = sc.transform(cv_data)
#test_data = sc.transform(test_data)

#Find out the parameters Mu and Covariance for passing to the probability density function
mu,sigma = estimateGaussian(train_data)

#Multivariate Gaussian distribution - This calculates the probability for each record.
p_train = MultivariateGaussianDistribution(train_data,mu,sigma)
print(p_train.mean())
print(p_train.std())
print(p_train.max())
print(p_train.min())

#Calculate the probabilities for cross validation and test records by passing the mean and co-variance matrix derived from train data
p_cv = MultivariateGaussianDistribution(cv_data,mu,sigma)
p_test = MultivariateGaussianDistribution(test_data,mu,sigma)

#Let us use cross validation to find the best threshold where the F1 -score is high
SelectThresholdByCV_Anomaly(p_cv,cv_data_y)

#CV data - Predictions
pred_cv= (p_cv < 0.2425112365126434)
Print_Accuracy_Scores(cv_data_y, pred_cv)

#Confusion matrix on CV
cnf_matrix = confusion_matrix(cv_data_y,pred_cv)
row_sum = cnf_matrix.sum(axis=1,keepdims=True)
cnf_matrix_norm =cnf_matrix / row_sum
sns.heatmap(cnf_matrix_norm,cmap='YlGnBu',annot=True)
plt.title("Normalized Confusion Matrix - Cross Validation")

"""Please notice that False negatives are around 24%. We tried to reduce this false negatives thus improve recall score by increasing the epsilon. We are successful in bringing the recall above 80%, however precsion is going down to 70% pretty quickly. Hence we decided to keep epsilon with best f1-score, i.e: 0.2425"""

#Test data - Check the F1-score by using the best threshold from cross validation
predictions = (p_test < 0.2425112365126434)
Print_Accuracy_Scores(test_data_y, predictions)

cnf_matrix = confusion_matrix(test_data_y, predictions)
row_sum = cnf_matrix.sum(axis=1,keepdims=True)
cnf_matrix_norm =cnf_matrix / row_sum
sns.heatmap(cnf_matrix_norm,cmap='YlGnBu',annot=True)
plt.title("Normalized Confusion Matrix - Test data")

"""Conclusion: Anomaly detection algorthm has provided decent results with F1-score of 83. We can improve recall & thus f1-score further by deriving new features based on the business knowledge. Since the features are transformed from PCA output, we couldn't understand their purpose and do feature engineering here.

Note: This is the best approach for handling imbalanced datasets and finding anomalies as per Professor Andrew Ng's course. So I believe we can explore a lot on this anomaly detection algorithm.

On a general note we could also use an unsupervised learning algorithm like K-means clustering to identify the clusters in the dataset, which could improve the predictive power in fraud detection (applicable for all 3 approaches we discussed).

Since the dataset is imbalanced we should apply clustering algorithm independently to minority and majority class instances.
Here is an example how we could proceed for clustering:

1) Majority Class: 7 Clusters with around 40,000 observations in each cluster
2) Minority Class: 3 Clusters with around 150 observations in each cluster

I leave the discussion on this note for now...
"""